{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKzLcYV0vh2E"
      },
      "outputs": [],
      "source": [
        "# HDI_v2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import numpy as np\n",
        "\n",
        "# Define the target correlations\n",
        "target_correlations = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
        "\n",
        "def rho_to_mi(rho, dim):\n",
        "    result = -dim / 2 * np.log(1 - rho **2)\n",
        "    return result\n",
        "\n",
        "##\n",
        "target_MIs=[]\n",
        "for rho in target_correlations:\n",
        "    mi = rho_to_mi(rho,dim=1)\n",
        "    target_MIs.append(round(mi, 2))\n",
        "\n",
        "# print('target_MIs= ',target_MIs) # target_MIs=  [0.83, 0.34, 0.14, 0.05, 0.01]\n",
        "\n",
        "\n",
        "# Define the correlation loss\n",
        "def correlation_loss(tensor, target_correlations):\n",
        "    level_1 = tensor[0]  # Level 1\n",
        "    loss = 0.0\n",
        "    correlations = []\n",
        "    for i, target_corr in enumerate(target_correlations):\n",
        "        level_i = tensor[i + 1]  # Other levels\n",
        "        level_1_flat = level_1.flatten(start_dim=0, end_dim=1)\n",
        "        level_i_flat = level_i.flatten(start_dim=0, end_dim=1)\n",
        "\n",
        "        # Compute Pearson correlation\n",
        "        cov = torch.mean((level_1_flat - level_1_flat.mean()) * (level_i_flat - level_i_flat.mean()))\n",
        "        std_1 = level_1_flat.std()\n",
        "        std_i = level_i_flat.std()\n",
        "        corr = cov / (std_1 * std_i + 1e-8)\n",
        "        correlations.append(corr.item())  # Store correlation\n",
        "        loss += (corr - target_corr) ** 2\n",
        "    return loss, correlations\n",
        "\n",
        "# Define the independence loss\n",
        "def independence_loss(tensor):\n",
        "    loss = 0.0\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    avg_correlations = []\n",
        "    for level in tensor:\n",
        "        level_corrs = []\n",
        "        for i in range(features):\n",
        "            for j in range(i + 1, features):\n",
        "                col_i = level[:, i]\n",
        "                col_j = level[:, j]\n",
        "                cov = torch.mean((col_i - col_i.mean()) * (col_j - col_j.mean()))\n",
        "                std_i = col_i.std()\n",
        "                std_j = col_j.std()\n",
        "                corr = cov / (std_i * std_j + 1e-8)\n",
        "                level_corrs.append(corr.item())  # Store correlation\n",
        "                loss += corr ** 2  # Penalize non-zero correlations\n",
        "        avg_correlations.append(sum(level_corrs) / len(level_corrs))  # Average correlation for this level\n",
        "    return loss, avg_correlations\n",
        "\n",
        "# Define the mutual information loss\n",
        "# def mutual_information_loss(tensor, target_MIs):\n",
        "#     level_1 = tensor[0].detach().cpu().numpy()  # Level 1\n",
        "#     loss = 0.0\n",
        "#     mutual_infos = []\n",
        "#     for i, target_corr in enumerate(target_MIs):\n",
        "#         level_i = tensor[i + 1].detach().cpu().numpy()  # Other levels\n",
        "\n",
        "#         # Flatten the tensors for mutual information calculation\n",
        "#         level_1_flat = level_1.reshape(-1)\n",
        "#         level_i_flat = level_i.reshape(-1)\n",
        "\n",
        "#         # Compute mutual information using sklearn\n",
        "#         mi = mutual_info_regression(level_1_flat.reshape(-1, 1), level_i_flat, random_state=42)\n",
        "#         mi_value = mi[0] / np.log(2)  # Normalize MI to the range [0, 1]\n",
        "\n",
        "#         mutual_infos.append(mi_value)  # Store normalized mutual information\n",
        "#         loss += (mi_value - target_corr) ** 2  # Penalize deviation from target\n",
        "#     return loss, mutual_infos\n",
        "\n",
        "def mutual_information_loss(tensor, target_MIs):\n",
        "    level_1 = tensor[0].detach().cpu().numpy()  # Level 1\n",
        "    loss = 0.0\n",
        "    mutual_infos = []\n",
        "    lambda_reg = 0.01  # Regularization parameter\n",
        "\n",
        "    for i, target_corr in enumerate(target_MIs):\n",
        "        level_i = tensor[i + 1].detach().cpu().numpy()  # Other levels\n",
        "\n",
        "        # Flatten the tensors for mutual information calculation\n",
        "        level_1_flat = level_1.reshape(-1)\n",
        "        level_i_flat = level_i.reshape(-1)\n",
        "\n",
        "        # Compute mutual information using sklearn\n",
        "        mi = mutual_info_regression(level_1_flat.reshape(-1, 1), level_i_flat, random_state=42)\n",
        "        mi_value = mi[0] / np.log(2)  # Normalize MI to the range [0, 1]\n",
        "\n",
        "        mutual_infos.append(mi_value)  # Store normalized mutual information\n",
        "        weight = 1.0 if mi_value < target_corr else 0.5  # Dynamic weighting\n",
        "        loss += weight * (mi_value - target_corr) ** 2  # Penalize deviation from target\n",
        "\n",
        "    # Add regularization\n",
        "    loss += lambda_reg * np.sum(np.square(mutual_infos))  # L2 regularization\n",
        "\n",
        "    return loss, mutual_infos\n",
        "\n",
        "\n",
        "# Compute mutual information between all pairs of levels\n",
        "def mutual_information_between_levels(tensor):\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    mi_matrix = np.zeros((levels, levels))  # MI matrix to store MI for all pairs of levels\n",
        "\n",
        "    # Compute MI for each pair of levels\n",
        "    for i in range(levels):\n",
        "        for j in range(i + 1, levels):  # Only compute for i < j to avoid redundancy\n",
        "            level_i = tensor[i].detach().cpu().numpy().reshape(-1)  # Flatten level i\n",
        "            level_j = tensor[j].detach().cpu().numpy().reshape(-1)  # Flatten level j\n",
        "\n",
        "            # Compute mutual information using sklearn\n",
        "            mi = mutual_info_regression(level_i.reshape(-1, 1), level_j, random_state=42)\n",
        "            mi_value = mi[0] / np.log(2)  # Normalize MI to the range [0, 1]\n",
        "\n",
        "            # Store MI value in the matrix\n",
        "            mi_matrix[i, j] = mi_value\n",
        "            mi_matrix[j, i] = mi_value  # Symmetric matrix\n",
        "\n",
        "    return mi_matrix\n",
        "\n",
        "# Define the model\n",
        "class CorrelationModel(nn.Module):\n",
        "    def __init__(self, levels, batch_size, features):\n",
        "        super(CorrelationModel, self).__init__()\n",
        "        self.levels = levels\n",
        "        self.batch_size = batch_size\n",
        "        self.features = features\n",
        "        self.transform = nn.Parameter(torch.randn(levels, batch_size, features))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.transform\n",
        "\n",
        "# Hyperparameters\n",
        "levels = 6\n",
        "batch_size = 256\n",
        "features = 5\n",
        "learning_rate = 0.0025\n",
        "num_epochs = 500\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = CorrelationModel(levels, batch_size, features)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track metrics for plotting\n",
        "# Track metrics for plotting\n",
        "correlation_history = []  # To track correlations between levels\n",
        "independence_history = []  # To track average independence correlations\n",
        "mutual_information_history = []  # To track mutual information between levels\n",
        "\n",
        "# Track MI between all pairs of levels\n",
        "mi_between_levels_history = []  # To store MI matrices for all epochs\n",
        "\n",
        "\n",
        "# Initialize lists to track losses\n",
        "corr_loss_history = []  # To track correlation loss\n",
        "indep_loss_history = []  # To track independence loss\n",
        "total_loss_history = []  # To track total loss\n",
        "mi_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model()  # Forward pass\n",
        "\n",
        "    # Compute the losses\n",
        "    corr_loss, correlations = correlation_loss(output, target_correlations)\n",
        "    indep_loss, avg_independence_corrs = independence_loss(output)\n",
        "    mi_loss, mutual_infos = mutual_information_loss(output, target_MIs)\n",
        "\n",
        "    # Compute MI between all pairs of levels\n",
        "    mi_matrix = mutual_information_between_levels(output)\n",
        "    mi_between_levels_history.append(mi_matrix)  # Store MI matrix for this epoch\n",
        "\n",
        "    # Combine losses\n",
        "    # total_loss = corr_loss + 0.1 * indep_loss + 0.1 * mi_loss\n",
        "    total_loss = corr_loss + 0.1 * indep_loss +  mi_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Append metrics to history lists\n",
        "    correlation_history.append(correlations)  # Append correlations for this epoch\n",
        "    independence_history.append(avg_independence_corrs)  # Append independence correlations for this epoch\n",
        "    mutual_information_history.append(mutual_infos)  # Append mutual information for this epoch\n",
        "\n",
        "\n",
        "    # Track losses\n",
        "    corr_loss_history.append(corr_loss.item())\n",
        "    indep_loss_history.append(indep_loss.item())\n",
        "    total_loss_history.append(total_loss.item())\n",
        "    mi_loss_history.append(mi_loss.item())\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], Correlation Loss: {corr_loss.item():.4f}, Independence Loss: {indep_loss.item():.4f}, Mutual Information Loss: {mi_loss:.4f}, Total Loss: {total_loss.item():.4f}\")\n",
        "        # print(f\"Correlations between Level 1 and other levels: {correlations}\")\n",
        "        # print(f\"Mutual Information between Level 1 and other levels: {mutual_infos}\")\n",
        "        # print(f\"Average independence correlations (within levels): {avg_independence_corrs}\")\n",
        "        # print(f\"Mutual Information Matrix (Levels):\\n{mi_matrix}\")\n",
        "\n",
        "\n",
        "\n",
        "##########################\n",
        "fig, axs = plt.subplots(1, 4, figsize=(18, 6))\n",
        "\n",
        "# Plot Correlation Loss\n",
        "axs[0].plot(corr_loss_history, label='MI Loss', color='blue', linewidth=4)\n",
        "axs[0].set_title(\"Correlation Loss Over Epochs\")\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "\n",
        "# Plot Independence Loss\n",
        "axs[1].plot(indep_loss_history, label='Independence Loss', color='orange', linewidth=4)\n",
        "axs[1].set_title(\"Independence Loss Over Epochs\")\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "\n",
        "# Plot Total Loss\n",
        "axs[2].plot(total_loss_history, label='Total Loss', color='green', linewidth=4)\n",
        "axs[2].set_title(\"Total Loss Over Epochs\")\n",
        "axs[2].set_xlabel(\"Epochs\")\n",
        "axs[2].set_ylabel(\"Loss\")\n",
        "axs[2].legend()\n",
        "axs[2].grid()\n",
        "\n",
        "# Plot MI Loss\n",
        "axs[3].plot(mi_loss_history, label='Total Loss', color='green', linewidth=4)\n",
        "axs[3].set_title(\"MI Loss Over Epochs\")\n",
        "axs[3].set_xlabel(\"Epochs\")\n",
        "axs[3].set_ylabel(\"Loss\")\n",
        "axs[3].legend()\n",
        "axs[3].grid()\n",
        "\n",
        "#################################################\n",
        "# Convert tracked metrics to tensors for easier plotting\n",
        "correlation_history = torch.tensor(correlation_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "independence_history = torch.tensor(independence_history)  # Shape: (num_epochs, levels)\n",
        "mutual_information_history = torch.tensor(mutual_information_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "\n",
        "\n",
        "# Plot correlations between levels\n",
        "##################################\n",
        "# Plot correlations between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_correlations)):\n",
        "    plt.plot(correlation_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_correlations[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label=\"Target Correlation (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.7, color='g', linestyle='--', label=\"Target Correlation (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.5, color='b', linestyle='--', label=\"Target Correlation (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.3, color='orange', linestyle='--', label=\"Target Correlation (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.1, color='purple', linestyle='--', label=\"Target Correlation (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"Correlation Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "##########################################\n",
        "# Plot average independence correlations within levels\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(levels):\n",
        "    plt.plot(independence_history[:, i], label=f\"Level {i + 1}\", linewidth=2)  # Adjust the linewidth as needed\n",
        "\n",
        "plt.title(\"Average Independence Correlations (Within Levels) Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Average Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "########################################\n",
        "# Plot mutual information between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_MIs)):\n",
        "    plt.plot(mutual_information_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_MIs[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.83, color='r', linestyle='--', label=\"Target MI (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.34, color='g', linestyle='--', label=\"Target MI (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.14, color='b', linestyle='--', label=\"Target MI (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.05, color='orange', linestyle='--', label=\"Target MI (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.01, color='purple', linestyle='--', label=\"Target MI (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"Correlation Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "import seaborn as sns\n",
        "#\n",
        "# Plot MI heatmap for the last epoch\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(mi_between_levels_history[-1], annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=[f\"Level {i+1}\" for i in range(levels)], yticklabels=[f\"Level {i+1}\" for i in range(levels)])\n",
        "plt.title(\"Mutual Information Between Levels (Last Epoch)\")\n",
        "plt.xlabel(\"Levels\")\n",
        "plt.ylabel(\"Levels\")\n",
        "plt.show()\n",
        "\n",
        "###########################################\n",
        "# Extract MI for all pairs of levels over epochs\n",
        "level_pairs = [(i, j) for i in range(levels) for j in range(i + 1, levels)]  # All unique pairs of levels\n",
        "mi_over_epochs = {pair: [mi_matrix[pair[0], pair[1]] for mi_matrix in mi_between_levels_history] for pair in level_pairs}\n",
        "\n",
        "# Plot MI for each pair over epochs\n",
        "plt.figure(figsize=(12, 6))\n",
        "for pair, mi_values in mi_over_epochs.items():\n",
        "    plt.plot(mi_values, label=f\"Level {pair[0] + 1} vs Level {pair[1] + 1}\", linewidth=2)  # Set linewidth to 2\n",
        "plt.title(\"Mutual Information Between Level Pairs Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mutual Information\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Define adjacent level pairs\n",
        "adjacent_level_pairs = [(i, i + 1) for i in range(levels - 1)]  # e.g., (1, 2), (2, 3), ..., (n-1, n)\n",
        "\n",
        "# Extract MI for adjacent pairs of levels over epochs\n",
        "mi_over_epochs_adjacent = {pair: [mi_matrix[pair[0], pair[1]] for mi_matrix in mi_between_levels_history] for pair in adjacent_level_pairs}\n",
        "\n",
        "# Plot MI for each adjacent pair over epochs\n",
        "plt.figure(figsize=(12, 6))\n",
        "for pair, mi_values in mi_over_epochs_adjacent.items():\n",
        "    plt.plot(mi_values, label=f\"Level {pair[0] + 1} vs Level {pair[1] + 1}\", linewidth=2)  # Set linewidth to 2\n",
        "plt.title(\"Mutual Information Between Adjacent Level Pairs Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mutual Information\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ]
}
