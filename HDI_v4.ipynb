{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBdDO3Ldo93-"
      },
      "outputs": [],
      "source": [
        "# HDI-v4 - AE-enabeld ==> Using fixed X as input\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_regression  # For mutual information calculation\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "\n",
        "# Define the target correlations\n",
        "target_correlations = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
        "\n",
        "# Define target dependence values (adjust as needed)\n",
        "target_dependence = [0.85, 0.65, 0.45, 0.25, 0.15]  # Example values\n",
        "\n",
        "# Under the Gaussian distribution, the correlation coefficient and mutual information have a one-to-one mapping:\n",
        "def rho_to_mi(rho, dim):\n",
        "    result = -dim / 2 * np.log(1 - rho ** 2)\n",
        "    return result\n",
        "\n",
        "def mi_to_rho(mi, dim):\n",
        "    result = np.sqrt(1 - np.exp(-2 * mi / dim))\n",
        "    return result\n",
        "\n",
        "# Calculate target mutual information values from target correlations\n",
        "target_MIs = []\n",
        "for rho in target_correlations:\n",
        "    mi = rho_to_mi(rho, dim=1)  # Assuming 1-dimensional features\n",
        "    target_MIs.append(round(mi, 2))\n",
        "\n",
        "print('Target Mutual Information (MIs):', target_MIs)  # Debugging: Print target MIs\n",
        "\n",
        "############################################################# Losses\n",
        "# Define the correlation loss\n",
        "def correlation_loss(tensor, target_correlations):\n",
        "    level_1 = tensor[0]  # Level 1\n",
        "    loss = 0.0\n",
        "    correlations = []\n",
        "    for i, target_corr in enumerate(target_correlations):\n",
        "        level_i = tensor[i + 1]  # Other levels\n",
        "        level_1_flat = level_1.flatten(start_dim=0, end_dim=1)\n",
        "        level_i_flat = level_i.flatten(start_dim=0, end_dim=1)\n",
        "\n",
        "        # Compute Pearson correlation\n",
        "        cov = torch.mean((level_1_flat - level_1_flat.mean()) * (level_i_flat - level_i_flat.mean()))\n",
        "        std_1 = level_1_flat.std()\n",
        "        std_i = level_i_flat.std()\n",
        "        corr = cov / (std_1 * std_i + 1e-8)\n",
        "        correlations.append(corr.item())  # Store correlation\n",
        "        loss += (corr - target_corr) ** 2\n",
        "\n",
        "    return loss, correlations\n",
        "\n",
        "# Define the independence loss\n",
        "def independence_loss(tensor):\n",
        "    loss = 0.0\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    avg_correlations = []\n",
        "    for level in tensor:\n",
        "        level_corrs = []\n",
        "        for i in range(features):\n",
        "            for j in range(i + 1, features):\n",
        "                col_i = level[:, i]\n",
        "                col_j = level[:, j]\n",
        "                cov = torch.mean((col_i - col_i.mean()) * (col_j - col_j.mean()))\n",
        "                std_i = col_i.std()\n",
        "                std_j = col_j.std()\n",
        "                corr = cov / (std_i * std_j + 1e-8)\n",
        "                level_corrs.append(corr.item())  # Store correlation\n",
        "                loss += corr ** 2  # Penalize non-zero correlations\n",
        "        avg_correlations.append(sum(level_corrs) / len(level_corrs))  # Average correlation for this level\n",
        "    return loss, avg_correlations\n",
        "\n",
        "# Define the mutual information loss\n",
        "def mutual_information_loss(tensor, target_MIs):\n",
        "    level_1 = tensor[0].detach().cpu().numpy()  # Level 1\n",
        "    loss = 0.0\n",
        "    mutual_infos = []\n",
        "    lambda_reg = 0.01  # Regularization parameter\n",
        "\n",
        "    for i, target_corr in enumerate(target_MIs):\n",
        "        level_i = tensor[i + 1].detach().cpu().numpy()  # Other levels\n",
        "\n",
        "        # Flatten the tensors for mutual information calculation\n",
        "        level_1_flat = level_1.reshape(-1)\n",
        "        level_i_flat = level_i.reshape(-1)\n",
        "\n",
        "        # Compute mutual information using sklearn\n",
        "        mi = mutual_info_regression(level_1_flat.reshape(-1, 1), level_i_flat, random_state=42)\n",
        "        mi_value = mi[0] / np.log(2)  # Normalize MI to the range [0, 1]\n",
        "\n",
        "        mutual_infos.append(mi_value)  # Store normalized mutual information\n",
        "        weight = 1.0 if mi_value < target_corr else 0.5  # Dynamic weighting\n",
        "        loss += weight * (mi_value - target_corr) ** 2  # Penalize deviation from target\n",
        "\n",
        "    # Add regularization\n",
        "    loss += lambda_reg * np.sum(np.square(mutual_infos))  # L2 regularization\n",
        "\n",
        "    return loss, mutual_infos\n",
        "\n",
        "# Define the distance correlation function\n",
        "def distance_correlation(x, y):\n",
        "    x = x - x.mean()\n",
        "    y = y - y.mean()\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    a = torch.cdist(x.unsqueeze(0), x.unsqueeze(0), p=2).squeeze()\n",
        "    b = torch.cdist(y.unsqueeze(0), y.unsqueeze(0), p=2).squeeze()\n",
        "\n",
        "    # Double centering\n",
        "    A = a - a.mean(dim=0) - a.mean(dim=1).unsqueeze(1) + a.mean()\n",
        "    B = b - b.mean(dim=0) - b.mean(dim=1).unsqueeze(1) + b.mean()\n",
        "\n",
        "    # Compute distance covariance, variance, and correlation\n",
        "    dcov = torch.sqrt((A * B).mean())\n",
        "    dvar_x = torch.sqrt((A * A).mean())\n",
        "    dvar_y = torch.sqrt((B * B).mean())\n",
        "\n",
        "    return dcov / (torch.sqrt(dvar_x * dvar_y) + 1e-8)\n",
        "\n",
        "# Define the adjacent level dependence loss\n",
        "def adjacent_level_dependence_loss(tensor, target_dependence):\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    loss = 0.0\n",
        "    dependence_values = []\n",
        "\n",
        "    for i in range(levels - 1):\n",
        "        level_i = tensor[i].reshape(-1, features)\n",
        "        level_next = tensor[i + 1].reshape(-1, features)\n",
        "\n",
        "        dcorr = distance_correlation(level_i, level_next)\n",
        "        dependence_values.append(dcorr.item())\n",
        "\n",
        "        # Compare to target dependence\n",
        "        if i < len(target_dependence):  # Ensure we have a target value\n",
        "            loss += (dcorr - target_dependence[i])**2\n",
        "\n",
        "    return loss, dependence_values\n",
        "\n",
        "\n",
        "######################################################  Models\n",
        "# Define the hierarchical bottleneck model\n",
        "class CorrelationModel(nn.Module):\n",
        "    def __init__(self, levels, features):\n",
        "        super(CorrelationModel, self).__init__()\n",
        "        self.levels = levels\n",
        "        self.features = features\n",
        "        self.linear_layers = nn.ModuleList([\n",
        "            nn.Linear(features, features) for _ in range(levels - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, encoder_output):\n",
        "        levels = [encoder_output]  # First level is the encoder output\n",
        "        for layer in self.linear_layers:\n",
        "            levels.append(layer(levels[-1]))  # Transform the previous level\n",
        "        return torch.stack(levels)  # Stack levels into a tensor\n",
        "\n",
        "# Define the Autoencoder\n",
        "class HierarchicalAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, levels, features):\n",
        "        super(HierarchicalAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, features),  # Output matches bottleneck feature size\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.bottleneck = CorrelationModel(levels, features)  # Hierarchical bottleneck\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()  # Assuming input is normalized between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        bottleneck = self.bottleneck(encoded)  # Hierarchical bottleneck\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, bottleneck, decoded\n",
        "\n",
        "\n",
        "###################################################### Configuration\n",
        "# Hyperparameters\n",
        "input_dim = 20  # Example input dimension\n",
        "hidden_dim = 64\n",
        "levels = 6\n",
        "features = 5\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = HierarchicalAutoencoder(input_dim, hidden_dim, levels, features)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "reconstruction_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Track metrics for plotting\n",
        "correlation_history = []  # To track correlations between levels\n",
        "independence_history = []  # To track average independence correlations\n",
        "mutual_information_history = []  # To track mutual information between levels\n",
        "adjacent_dependence_history = []  # To track adjacent level dependence\n",
        "\n",
        "# Initialize lists to track losses\n",
        "corr_loss_history = []  # To track correlation loss\n",
        "indep_loss_history = []  # To track independence loss\n",
        "total_loss_history = []  # To track total loss\n",
        "mi_loss_history = []\n",
        "adj_loss_history=[]\n",
        "\n",
        "x = torch.randn(256, input_dim)  # Batch size = 256\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    encoded, bottleneck, decoded = model(x)\n",
        "\n",
        "    # Compute the losses\n",
        "    reconstruction_loss = reconstruction_loss_fn(decoded, x)\n",
        "    corr_loss, correlations = correlation_loss(bottleneck, target_correlations)\n",
        "    indep_loss, avg_independence_corrs = independence_loss(bottleneck)\n",
        "    mi_loss, mutual_infos = mutual_information_loss(bottleneck, target_MIs)\n",
        "    adj_loss, dependence_values = adjacent_level_dependence_loss(bottleneck, target_dependence)\n",
        "\n",
        "    total_loss = reconstruction_loss + corr_loss + 0.1 * indep_loss + 0.1 * mi_loss + 0.1 * adj_loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track metrics\n",
        "    correlation_history.append(correlations)\n",
        "    independence_history.append(avg_independence_corrs)\n",
        "    mutual_information_history.append(mutual_infos)\n",
        "    adjacent_dependence_history.append(dependence_values)\n",
        "\n",
        "\n",
        "    # Track losses\n",
        "    corr_loss_history.append(corr_loss.item())\n",
        "    indep_loss_history.append(indep_loss.item())\n",
        "    total_loss_history.append(total_loss.item())\n",
        "    mi_loss_history.append(mi_loss.item())\n",
        "    adj_loss_history.append(adj_loss.item())\n",
        "\n",
        "        # Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Reconstruction Loss: {reconstruction_loss.item():.4f}, \"\n",
        "              f\"Correlation Loss: {corr_loss.item():.4f}, Independence Loss: {indep_loss:.4f}, \"\n",
        "              f\"Mutual Information Loss: {mi_loss:.4f}, Adjacent Dependence Loss: {adj_loss:.4f}, \"\n",
        "              f\"Total Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "############################################ Plotting\n",
        "###\n",
        "import matplotlib.pyplot as plt\n",
        "# Import necessary libraries\n",
        "fig, axs = plt.subplots(1, 5, figsize=(18, 6))\n",
        "\n",
        "# Plot Correlation Loss\n",
        "axs[0].plot(corr_loss_history, label='MI Loss', color='blue', linewidth=4)\n",
        "axs[0].set_title(\"Correlation Loss Over Epochs\")\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "\n",
        "# Plot Independence Loss\n",
        "axs[1].plot(indep_loss_history, label='Independence Loss', color='orange', linewidth=4)\n",
        "axs[1].set_title(\"Independence Loss Over Epochs\")\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "\n",
        "\n",
        "\n",
        "# Plot MI Loss\n",
        "axs[2].plot(mi_loss_history, label='Total Loss', color='red', linewidth=4)\n",
        "axs[2].set_title(\"MI Loss Over Epochs\")\n",
        "axs[2].set_xlabel(\"Epochs\")\n",
        "axs[2].set_ylabel(\"Loss\")\n",
        "axs[2].legend()\n",
        "axs[2].grid()\n",
        "\n",
        "# Plot Adjacent Level Loss\n",
        "axs[3].plot(adj_loss_history, label='Total Loss', color='magenta', linewidth=4)\n",
        "axs[3].set_title(\"Adjacent Level Loss Over Epochs\")\n",
        "axs[3].set_xlabel(\"Epochs\")\n",
        "axs[3].set_ylabel(\"Loss\")\n",
        "axs[3].legend()\n",
        "axs[3].grid()\n",
        "\n",
        "# Plot Total Loss\n",
        "axs[-1].plot(total_loss_history, label='Total Loss', color='green', linewidth=4)\n",
        "axs[-1].set_title(\"Total Loss Over Epochs\")\n",
        "axs[-1].set_xlabel(\"Epochs\")\n",
        "axs[-1].set_ylabel(\"Loss\")\n",
        "axs[-1].legend()\n",
        "axs[-1].grid()\n",
        "\n",
        "\n",
        "# #################################################\n",
        "# Convert tracked metrics to tensors for easier plotting\n",
        "correlation_history = torch.tensor(correlation_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "independence_history = torch.tensor(independence_history)  # Shape: (num_epochs, levels)\n",
        "mutual_information_history = torch.tensor(mutual_information_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "adjacent_dependence_history=torch.tensor(adjacent_dependence_history)\n",
        "\n",
        "\n",
        "# # # Plot correlations between levels\n",
        "# # ##################################\n",
        "# # # Plot correlations between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_correlations)):\n",
        "    plt.plot(correlation_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_correlations[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label=\"Target Correlation (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.7, color='g', linestyle='--', label=\"Target Correlation (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.5, color='b', linestyle='--', label=\"Target Correlation (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.3, color='orange', linestyle='--', label=\"Target Correlation (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.1, color='purple', linestyle='--', label=\"Target Correlation (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"Correlation Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# # ##########################################\n",
        "# # # Plot average independence correlations within levels\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(levels):\n",
        "    plt.plot(independence_history[:, i], label=f\"Level {i + 1}\", linewidth=2)  # Adjust the linewidth as needed\n",
        "\n",
        "plt.title(\"Average Independence Correlations (Within Levels) Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Average Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # ########################################\n",
        "# # # Plot mutual information between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_MIs)):\n",
        "    plt.plot(mutual_information_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_MIs[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.83, color='r', linestyle='--', label=\"Target MI (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.34, color='g', linestyle='--', label=\"Target MI (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.14, color='b', linestyle='--', label=\"Target MI (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.05, color='orange', linestyle='--', label=\"Target MI (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.01, color='purple', linestyle='--', label=\"Target MI (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"MI Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MI\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Adjacent Level MIs\n",
        "# Plot dependence between adjacent levels\n",
        "# Define colors for the lines\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to levels\n",
        "\n",
        "# Plot dependence between adjacent levels\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create lists to hold handles and labels for the legend\n",
        "handles = []\n",
        "labels = []\n",
        "\n",
        "# Plot dependence lines\n",
        "for i in range(levels - 1):\n",
        "    dependence_by_level = [dependence[i] for dependence in adjacent_dependence_history]\n",
        "    # Plot dependence lines\n",
        "    line, = plt.plot(dependence_by_level, linewidth=4, color=colors[i], label=f'Dependence (Level {i + 1} vs Level {i + 2})')\n",
        "    handles.append(line)  # Store the line handle for the legend\n",
        "    labels.append(f'Dependence (Level {i + 1} vs Level {i + 2})')  # Store the label\n",
        "\n",
        "# Plot target lines with corresponding colors\n",
        "for i in range(levels - 1):\n",
        "    target_line = plt.axhline(y=target_dependence[i], color=colors[i], linestyle='--', linewidth=2, label=f'Target Dependence (Level {i + 2})')\n",
        "    handles.append(target_line)  # Store the target line handle for the legend\n",
        "    labels.append(f'Target Dependence (Level {i + 2})')  # Store the label\n",
        "\n",
        "plt.title('Dependence Between Adjacent Levels')\n",
        "plt.xlabel('Epochs')  # You can keep this label or change it as needed\n",
        "plt.ylabel('Dependence distance_correlation')\n",
        "\n",
        "# Create a single legend in the top left corner of the plot\n",
        "plt.legend(handles, labels, loc='upper left')\n",
        "\n",
        "plt.grid()\n",
        "plt.tight_layout()  # Adjust layout to make room for the legend\n",
        "plt.show()"
      ]
    }
  ]
}