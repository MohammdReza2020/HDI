{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKzLcYV0vh2E"
      },
      "outputs": [],
      "source": [
        "# HDI-v3\n",
        "# 14030902\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import numpy as np\n",
        "\n",
        "# Define the target correlations\n",
        "target_correlations = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
        "# Define target dependence values (adjust as needed)\n",
        "\n",
        "\n",
        "target_dependence = [0.85, 0.65, 0.45, 0.25, 0.15]  # Example values\n",
        "# target_dependence = [0.5, 0.5, 0.5, 0.5, 0.5]  # Example values\n",
        "\n",
        "# Under the Gaussian distribution, the correlation coefficient and mutual information have one-to-one mapping:\n",
        "def rho_to_mi(rho, dim):\n",
        "    result = -dim / 2 * np.log(1 - rho **2)\n",
        "    return result\n",
        "\n",
        "\n",
        "def mi_to_rho(mi, dim):\n",
        "    result = np.sqrt(1 - np.exp(-2 * mi / dim))\n",
        "    return result\n",
        "\n",
        "\n",
        "##\n",
        "target_MIs=[]\n",
        "for rho in target_correlations:\n",
        "    mi = rho_to_mi(rho,dim=1)\n",
        "    target_MIs.append(round(mi, 2))\n",
        "\n",
        "# print('target_MIs= ',target_MIs) # target_MIs=  [0.83, 0.34, 0.14, 0.05, 0.01]\n",
        "\n",
        "# Define the correlation loss (no changes here)\n",
        "def correlation_loss(tensor, target_correlations):\n",
        "    level_1 = tensor[0]  # Level 1\n",
        "    loss = 0.0\n",
        "    correlations = []\n",
        "    for i, target_corr in enumerate(target_correlations):\n",
        "        level_i = tensor[i + 1]  # Other levels\n",
        "        level_1_flat = level_1.flatten(start_dim=0, end_dim=1)\n",
        "        level_i_flat = level_i.flatten(start_dim=0, end_dim=1)\n",
        "\n",
        "        # Compute Pearson correlation\n",
        "        cov = torch.mean((level_1_flat - level_1_flat.mean()) * (level_i_flat - level_i_flat.mean()))\n",
        "        std_1 = level_1_flat.std()\n",
        "        std_i = level_i_flat.std()\n",
        "        corr = cov / (std_1 * std_i + 1e-8)\n",
        "        correlations.append(corr.item())  # Store correlation\n",
        "        loss += (corr - target_corr) ** 2\n",
        "    return loss, correlations\n",
        "\n",
        "# Define the independence loss (no changes here)\n",
        "def independence_loss(tensor):\n",
        "    loss = 0.0\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    avg_correlations = []\n",
        "    for level in tensor:\n",
        "        level_corrs = []\n",
        "        for i in range(features):\n",
        "            for j in range(i + 1, features):\n",
        "                col_i = level[:, i]\n",
        "                col_j = level[:, j]\n",
        "                cov = torch.mean((col_i - col_i.mean()) * (col_j - col_j.mean()))\n",
        "                std_i = col_i.std()\n",
        "                std_j = col_j.std()\n",
        "                corr = cov / (std_i * std_j + 1e-8)\n",
        "                level_corrs.append(corr.item())  # Store correlation\n",
        "                loss += corr ** 2  # Penalize non-zero correlations\n",
        "        avg_correlations.append(sum(level_corrs) / len(level_corrs))  # Average correlation for this level\n",
        "    return loss, avg_correlations\n",
        "\n",
        "# Define the mutual information loss (no changes here)\n",
        "def mutual_information_loss(tensor, target_MIs):\n",
        "    level_1 = tensor[0].detach().cpu().numpy()  # Level 1\n",
        "    loss = 0.0\n",
        "    mutual_infos = []\n",
        "    lambda_reg = 0.01  # Regularization parameter\n",
        "\n",
        "    for i, target_corr in enumerate(target_MIs):\n",
        "        level_i = tensor[i + 1].detach().cpu().numpy()  # Other levels\n",
        "\n",
        "        # Flatten the tensors for mutual information calculation\n",
        "        level_1_flat = level_1.reshape(-1)\n",
        "        level_i_flat = level_i.reshape(-1)\n",
        "\n",
        "        # Compute mutual information using sklearn\n",
        "        mi = mutual_info_regression(level_1_flat.reshape(-1, 1), level_i_flat, random_state=42)\n",
        "        mi_value = mi[0] / np.log(2)  # Normalize MI to the range [0, 1]\n",
        "\n",
        "        mutual_infos.append(mi_value)  # Store normalized mutual information\n",
        "        weight = 1.0 if mi_value < target_corr else 0.5  # Dynamic weighting\n",
        "        loss += weight * (mi_value - target_corr) ** 2  # Penalize deviation from target\n",
        "\n",
        "    # Add regularization\n",
        "    loss += lambda_reg * np.sum(np.square(mutual_infos))  # L2 regularization\n",
        "\n",
        "    return loss, mutual_infos\n",
        "\n",
        "############################################################################\n",
        "# Define the distance correlation function\n",
        "def distance_correlation(x, y):\n",
        "    \"\"\"\n",
        "    Compute the distance correlation between two tensors x and y.\n",
        "    \"\"\"\n",
        "    # Center the data\n",
        "    x = x - x.mean()\n",
        "    y = y - y.mean()\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    a = torch.cdist(x.unsqueeze(0), x.unsqueeze(0), p=2).squeeze()\n",
        "    b = torch.cdist(y.unsqueeze(0), y.unsqueeze(0), p=2).squeeze()\n",
        "\n",
        "    # Double centering\n",
        "    A = a - a.mean(dim=0) - a.mean(dim=1).unsqueeze(1) + a.mean()\n",
        "    B = b - b.mean(dim=0) - b.mean(dim=1).unsqueeze(1) + b.mean()\n",
        "\n",
        "    # Compute distance covariance, variance, and correlation\n",
        "    dcov = torch.sqrt((A * B).mean())\n",
        "    dvar_x = torch.sqrt((A * A).mean())\n",
        "    dvar_y = torch.sqrt((B * B).mean())\n",
        "\n",
        "    # Return distance correlation\n",
        "    return dcov / (torch.sqrt(dvar_x * dvar_y) + 1e-8)\n",
        "\n",
        "\n",
        "# Define the adjacent level dependence loss using distance correlation\n",
        "# Modified adjacent_level_dependence_loss function\n",
        "def adjacent_level_dependence_loss(tensor, target_dependence):\n",
        "    \"\"\"\n",
        "    Ensure that dependence between adjacent levels is close to target values.\n",
        "    \"\"\"\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    loss = 0.0\n",
        "    dependence_values = []\n",
        "\n",
        "    for i in range(levels - 1):\n",
        "        level_i = tensor[i].reshape(-1, features)\n",
        "        level_next = tensor[i + 1].reshape(-1, features)\n",
        "\n",
        "        dcorr = distance_correlation(level_i, level_next)\n",
        "        dependence_values.append(dcorr.item())\n",
        "\n",
        "        # Compare to target dependence\n",
        "        if i < len(target_dependence):  # Ensure we have a target value\n",
        "            loss += (dcorr - target_dependence[i])**2\n",
        "\n",
        "    return loss, dependence_values\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "def hsic_loss(tensor, sigma=None):\n",
        "    \"\"\"\n",
        "    Calculates the HSIC loss between adjacent levels of a tensor.\n",
        "\n",
        "    Args:\n",
        "        tensor: Input tensor with shape (levels, batch_size, features).\n",
        "        sigma: Kernel width (float or None for automatic estimation).\n",
        "\n",
        "    Returns:\n",
        "        HSIC loss value (torch.Tensor).\n",
        "    \"\"\"\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    loss = 0.0\n",
        "    hsic_values = []\n",
        "\n",
        "    for i in range(levels - 1):\n",
        "        level_i = tensor[i].reshape(batch_size, features)\n",
        "        level_next = tensor[i + 1].reshape(batch_size, features)\n",
        "\n",
        "        if sigma is None:\n",
        "            # Heuristic for sigma based on median distance\n",
        "            dist_matrix = torch.cdist(level_i, level_i, p=2)\n",
        "            sigma_i = torch.median(dist_matrix) / np.sqrt(2 * np.log(batch_size))\n",
        "\n",
        "            dist_matrix = torch.cdist(level_next, level_next, p=2)\n",
        "            sigma_next = torch.median(dist_matrix) / np.sqrt(2 * np.log(batch_size))\n",
        "\n",
        "            sigma = (sigma_i + sigma_next) / 2\n",
        "\n",
        "        level_hsic = hsic_loss_level(level_i, level_next, sigma) # Call the level-wise HSIC function\n",
        "        loss += level_hsic  # Accumulate loss for each adjacent pair\n",
        "        hsic_values.append(level_hsic.item())\n",
        "\n",
        "    return loss, hsic_values\n",
        "\n",
        "def hsic_loss_level(x, y, sigma):\n",
        "    \"\"\"\n",
        "    Calculates the HSIC (Hilbert-Schmidt Independence Criterion) loss between two tensors at a specific level.\n",
        "\n",
        "    Args:\n",
        "        x: First tensor (torch.Tensor).\n",
        "        y: Second tensor (torch.Tensor).\n",
        "        sigma: Kernel width (float).\n",
        "\n",
        "    Returns:\n",
        "        HSIC value (torch.Tensor).\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "\n",
        "    # Gaussian kernel\n",
        "    def gaussian_kernel(z, sigma):\n",
        "        pairwise_dists = torch.cdist(z, z, p=2)\n",
        "        return torch.exp(-pairwise_dists**2 / (2 * sigma**2))\n",
        "\n",
        "    Kx = gaussian_kernel(x, sigma)\n",
        "    Ky = gaussian_kernel(y, sigma)\n",
        "\n",
        "    H = torch.eye(n) - (1.0 / n) * torch.ones((n, n), device=x.device)  # Centering matrix\n",
        "    HSIC = (1.0 / (n**2)) * torch.trace(Kx @ H @ Ky @ H)\n",
        "    return -HSIC  # Negative HSIC to maximize dependence\n",
        "\n",
        "\n",
        "\n",
        "def kcca_loss(tensor, n_components=1):\n",
        "    \"\"\"\n",
        "    Calculates the KCCA loss between adjacent levels of a tensor.\n",
        "\n",
        "    Args:\n",
        "        tensor: Input tensor with shape (levels, batch_size, features).\n",
        "        n_components: Number of canonical components to consider.\n",
        "\n",
        "    Returns:\n",
        "        KCCA loss value (torch.Tensor).\n",
        "    \"\"\"\n",
        "    levels, batch_size, features = tensor.shape\n",
        "    loss = 0.0\n",
        "    kcca_values = []\n",
        "\n",
        "    for i in range(levels - 1):\n",
        "        level_i = tensor[i].reshape(batch_size, features).detach().cpu().numpy()\n",
        "        level_next = tensor[i + 1].reshape(batch_size, features).detach().cpu().numpy()\n",
        "\n",
        "        cca = CCA(n_components=n_components)\n",
        "        cca.fit(level_i, level_next)\n",
        "        x_c, y_c = cca.transform(level_i, level_next)\n",
        "\n",
        "        correlations = []\n",
        "        for j in range(n_components):\n",
        "            corr = np.corrcoef(x_c[:, j], y_c[:, j])[0, 1]\n",
        "            correlations.append(corr)\n",
        "\n",
        "        level_kcca = -torch.tensor(sum(correlations)) # Negative sum to maximize correlations\n",
        "        loss += level_kcca\n",
        "        kcca_values.append(level_kcca.item())\n",
        "\n",
        "    return loss, kcca_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################################################\n",
        "# Define the model (no changes here)\n",
        "class CorrelationModel(nn.Module):\n",
        "    def __init__(self, levels, batch_size, features):\n",
        "        super(CorrelationModel, self).__init__()\n",
        "        self.levels = levels\n",
        "        self.batch_size = batch_size\n",
        "        self.features = features\n",
        "        # MRH:\n",
        "        self.transform = nn.Parameter(torch.randn(levels, batch_size, features))\n",
        "        #self.transform = nn.Parameter(torch.randn(levels, batch_size, features) * 0.1)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.transform\n",
        "\n",
        "# Hyperparameters\n",
        "levels = 6\n",
        "batch_size = 256\n",
        "features = 5\n",
        "learning_rate = 0.0025\n",
        "num_epochs = 500\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = CorrelationModel(levels, batch_size, features)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track metrics for plotting\n",
        "correlation_history = []  # To track correlations between levels\n",
        "independence_history = []  # To track average independence correlations\n",
        "mutual_information_history = []  # To track mutual information between levels\n",
        "adjacent_dependence_history = []  # To track adjacent level dependence\n",
        "\n",
        "# Initialize lists to track losses\n",
        "corr_loss_history = []  # To track correlation loss\n",
        "indep_loss_history = []  # To track independence loss\n",
        "total_loss_history = []  # To track total loss\n",
        "mi_loss_history = []\n",
        "adj_loss_history=[]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model()  # Forward pass\n",
        "\n",
        "    # Normalize the output at each level\n",
        "    normalized_output = []\n",
        "    for level_output in output:\n",
        "        mean = level_output.mean(dim=[0,1], keepdim=True) # Calculate mean and std across batch and features\n",
        "        std = level_output.std(dim=[0,1], keepdim=True)\n",
        "        normalized_level = (level_output - mean) / (std + 1e-8) # Normalize and handle potential zero std\n",
        "        normalized_output.append(normalized_level)\n",
        "    normalized_output = torch.stack(normalized_output) # Stack back into a single tensor\n",
        "\n",
        "    # Compute the losses\n",
        "    corr_loss, correlations = correlation_loss(normalized_output, target_correlations)\n",
        "    indep_loss, avg_independence_corrs = independence_loss(normalized_output)\n",
        "    mi_loss, mutual_infos = mutual_information_loss(normalized_output, target_correlations)\n",
        "    #adj_loss, dependence_values = adjacent_level_dependence_loss(normalized_output)\n",
        "    adj_loss, dependence_values = adjacent_level_dependence_loss(normalized_output, target_dependence) # Pass normalized output\n",
        "\n",
        "    #\n",
        "    # hsic, hsic_values = hsic_loss(normalized_output)\n",
        "    # kcca, kcca_values = kcca_loss(normalized_output)\n",
        "\n",
        "    # Combine losses\n",
        "    #MRH:\n",
        "    # total_loss = corr_loss + 0.1 * indep_loss + 0.1 * mi_loss + 0.1 * adj_loss\n",
        "    total_loss = corr_loss + 0.1 * indep_loss +  mi_loss + adj_loss\n",
        "    #total_loss = corr_loss + 0.1 * indep_loss + 0.1 * mi_loss -0.1* kcca # Example using HSIC\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Append metrics to history lists\n",
        "    correlation_history.append(correlations)\n",
        "    independence_history.append(avg_independence_corrs)\n",
        "    mutual_information_history.append(mutual_infos)\n",
        "    adjacent_dependence_history.append(dependence_values)\n",
        "    #adjacent_dependence_history.append(kcca_values)\n",
        "\n",
        "\n",
        "    # Track losses\n",
        "    corr_loss_history.append(corr_loss.item())\n",
        "    indep_loss_history.append(indep_loss.item())\n",
        "    total_loss_history.append(total_loss.item())\n",
        "    mi_loss_history.append(mi_loss.item())\n",
        "    adj_loss_history.append(adj_loss.item())\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], Correlation Loss: {corr_loss.item():.4f}, Independence Loss: {indep_loss.item():.4f}, Mutual Information Loss: {mi_loss:.4f}, Adjacent Dependence Loss: {adj_loss:.4f}, Total Loss: {total_loss.item():.4f}\")\n",
        "        # print(f\"Correlations between Level 1 and other levels: {correlations}\")\n",
        "        # print(f\"Mutual Information between Level 1 and other levels: {mutual_infos}\")\n",
        "        # print(f\"Average independence correlations (within levels): {avg_independence_corrs}\")\n",
        "        # print(f\"Dependence between adjacent levels: {dependence_values}\")\n",
        "\n",
        "###\n",
        "# Import necessary libraries\n",
        "fig, axs = plt.subplots(1, 5, figsize=(18, 6))\n",
        "\n",
        "# Plot Correlation Loss\n",
        "axs[0].plot(corr_loss_history, label='MI Loss', color='blue', linewidth=4)\n",
        "axs[0].set_title(\"Correlation Loss Over Epochs\")\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "\n",
        "# Plot Independence Loss\n",
        "axs[1].plot(indep_loss_history, label='Independence Loss', color='orange', linewidth=4)\n",
        "axs[1].set_title(\"Independence Loss Over Epochs\")\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "\n",
        "\n",
        "\n",
        "# Plot MI Loss\n",
        "axs[2].plot(mi_loss_history, label='Total Loss', color='red', linewidth=4)\n",
        "axs[2].set_title(\"MI Loss Over Epochs\")\n",
        "axs[2].set_xlabel(\"Epochs\")\n",
        "axs[2].set_ylabel(\"Loss\")\n",
        "axs[2].legend()\n",
        "axs[2].grid()\n",
        "\n",
        "# Plot Adjacent Level Loss\n",
        "axs[3].plot(adj_loss_history, label='Total Loss', color='magenta', linewidth=4)\n",
        "axs[3].set_title(\"Adjacent Level Loss Over Epochs\")\n",
        "axs[3].set_xlabel(\"Epochs\")\n",
        "axs[3].set_ylabel(\"Loss\")\n",
        "axs[3].legend()\n",
        "axs[3].grid()\n",
        "\n",
        "# Plot Total Loss\n",
        "axs[-1].plot(total_loss_history, label='Total Loss', color='green', linewidth=4)\n",
        "axs[-1].set_title(\"Total Loss Over Epochs\")\n",
        "axs[-1].set_xlabel(\"Epochs\")\n",
        "axs[-1].set_ylabel(\"Loss\")\n",
        "axs[-1].legend()\n",
        "axs[-1].grid()\n",
        "\n",
        "\n",
        "# #################################################\n",
        "# Convert tracked metrics to tensors for easier plotting\n",
        "correlation_history = torch.tensor(correlation_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "independence_history = torch.tensor(independence_history)  # Shape: (num_epochs, levels)\n",
        "mutual_information_history = torch.tensor(mutual_information_history)  # Shape: (num_epochs, len(target_correlations))\n",
        "adjacent_dependence_history=torch.tensor(adjacent_dependence_history)\n",
        "\n",
        "\n",
        "# # # Plot correlations between levels\n",
        "# # ##################################\n",
        "# # # Plot correlations between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_correlations)):\n",
        "    plt.plot(correlation_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_correlations[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label=\"Target Correlation (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.7, color='g', linestyle='--', label=\"Target Correlation (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.5, color='b', linestyle='--', label=\"Target Correlation (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.3, color='orange', linestyle='--', label=\"Target Correlation (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.1, color='purple', linestyle='--', label=\"Target Correlation (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"Correlation Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# # ##########################################\n",
        "# # # Plot average independence correlations within levels\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(levels):\n",
        "    plt.plot(independence_history[:, i], label=f\"Level {i + 1}\", linewidth=2)  # Adjust the linewidth as needed\n",
        "\n",
        "plt.title(\"Average Independence Correlations (Within Levels) Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Average Correlation\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # ########################################\n",
        "# # # Plot mutual information between levels\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to target correlations\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot each level with the corresponding color\n",
        "for i in range(len(target_MIs)):\n",
        "    plt.plot(mutual_information_history[:, i],\n",
        "             label=f\"Level {i + 2} (Target: {target_MIs[i]})\",\n",
        "             linewidth=2,\n",
        "             color=colors[i])  # Use the defined colors\n",
        "\n",
        "# Draw horizontal lines with corresponding colors\n",
        "plt.axhline(y=0.83, color='r', linestyle='--', label=\"Target MI (Level 2)\", linewidth=2)\n",
        "plt.axhline(y=0.34, color='g', linestyle='--', label=\"Target MI (Level 3)\", linewidth=2)\n",
        "plt.axhline(y=0.14, color='b', linestyle='--', label=\"Target MI (Level 4)\", linewidth=2)\n",
        "plt.axhline(y=0.05, color='orange', linestyle='--', label=\"Target MI (Level 5)\", linewidth=2)\n",
        "plt.axhline(y=0.01, color='purple', linestyle='--', label=\"Target MI (Level 6)\", linewidth=2)\n",
        "\n",
        "plt.title(\"MI Between Level 1 and Other Levels Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MI\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ##########################################\n",
        "# import seaborn as sns\n",
        "# #\n",
        "# # Plot MI heatmap for the last epoch\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(mi_between_levels_history[-1], annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=[f\"Level {i+1}\" for i in range(levels)], yticklabels=[f\"Level {i+1}\" for i in range(levels)])\n",
        "plt.title(\"Mutual Information Between Levels (Last Epoch)\")\n",
        "plt.xlabel(\"Levels\")\n",
        "plt.ylabel(\"Levels\")\n",
        "plt.show()\n",
        "\n",
        "# ###########################################\n",
        "# # Extract MI for all pairs of levels over epochs\n",
        "# level_pairs = [(i, j) for i in range(levels) for j in range(i + 1, levels)]  # All unique pairs of levels\n",
        "# mi_over_epochs = {pair: [mi_matrix[pair[0], pair[1]] for mi_matrix in mi_between_levels_history] for pair in level_pairs}\n",
        "\n",
        "# # Plot MI for each pair over epochs\n",
        "plt.figure(figsize=(12, 6))\n",
        "for pair, mi_values in mi_over_epochs.items():\n",
        "    plt.plot(mi_values, label=f\"Level {pair[0] + 1} vs Level {pair[1] + 1}\", linewidth=2)  # Set linewidth to 2\n",
        "plt.title(\"Mutual Information Between Level Pairs Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mutual Information\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# # Define adjacent level pairs\n",
        "# adjacent_level_pairs = [(i, i + 1) for i in range(levels - 1)]  # e.g., (1, 2), (2, 3), ..., (n-1, n)\n",
        "\n",
        "# # Extract MI for adjacent pairs of levels over epochs\n",
        "mi_over_epochs_adjacent = {pair: [mi_matrix[pair[0], pair[1]] for mi_matrix in mi_between_levels_history] for pair in adjacent_level_pairs}\n",
        "\n",
        "# Plot MI for each adjacent pair over epochs\n",
        "plt.figure(figsize=(12, 6))\n",
        "for pair, mi_values in mi_over_epochs_adjacent.items():\n",
        "    plt.plot(mi_values, label=f\"Level {pair[0] + 1} vs Level {pair[1] + 1}\", linewidth=2)  # Set linewidth to 2\n",
        "plt.title(\"Mutual Information Between Adjacent Level Pairs Over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mutual Information\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Adjacent Level MIs\n",
        "# Plot dependence between adjacent levels\n",
        "# Define colors for the lines\n",
        "colors = ['r', 'g', 'b', 'orange', 'purple']  # Corresponding to levels\n",
        "\n",
        "# Plot dependence between adjacent levels\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create lists to hold handles and labels for the legend\n",
        "handles = []\n",
        "labels = []\n",
        "\n",
        "# Plot dependence lines\n",
        "for i in range(levels - 1):\n",
        "    dependence_by_level = [dependence[i] for dependence in adjacent_dependence_history]\n",
        "    # Plot dependence lines\n",
        "    line, = plt.plot(dependence_by_level, linewidth=4, color=colors[i], label=f'Dependence (Level {i + 1} vs Level {i + 2})')\n",
        "    handles.append(line)  # Store the line handle for the legend\n",
        "    labels.append(f'Dependence (Level {i + 1} vs Level {i + 2})')  # Store the label\n",
        "\n",
        "# Plot target lines with corresponding colors\n",
        "for i in range(levels - 1):\n",
        "    target_line = plt.axhline(y=target_dependence[i], color=colors[i], linestyle='--', linewidth=2, label=f'Target Dependence (Level {i + 2})')\n",
        "    handles.append(target_line)  # Store the target line handle for the legend\n",
        "    labels.append(f'Target Dependence (Level {i + 2})')  # Store the label\n",
        "\n",
        "plt.title('Dependence Between Adjacent Levels')\n",
        "plt.xlabel('Epochs')  # You can keep this label or change it as needed\n",
        "plt.ylabel('Dependence distance_correlation')\n",
        "\n",
        "# Create a single legend in the top left corner of the plot\n",
        "plt.legend(handles, labels, loc='upper left')\n",
        "\n",
        "plt.grid()\n",
        "plt.tight_layout()  # Adjust layout to make room for the legend\n",
        "plt.show()"
      ]
    }
  ]
}